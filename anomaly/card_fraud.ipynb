{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    284315\n",
      "1       492\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# csv 파일 읽기\n",
    "df = pd.read_csv(\"creditcard.csv\")\n",
    "\n",
    "# 492 fraudulent transactions, 284,315 normal transactions.\n",
    "# 0.172% of transactions were fraud.\n",
    "print(df.Class.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud  :  492\n",
      "Normal :  284315\n"
     ]
    }
   ],
   "source": [
    "df.loc[df.Class == 0, 'Normal'] = 1\n",
    "df.loc[df.Class == 1, 'Normal'] = 0\n",
    "\n",
    "# Rename 'Class' to 'Fraud'\n",
    "# Class -> Fraud 로 명 변환\n",
    "df = df.rename(columns = {'Class' : 'Fraud'})\n",
    "\n",
    "\n",
    "# max column 수 설정\n",
    "pd.set_option('display.max_columns',  101)\n",
    "# print(df.head())\n",
    "\n",
    "# Create dataframes of only Fraud and Normal transactions.\n",
    "Fraud = df[df.Fraud == 1]\n",
    "Normal = df[df.Normal == 1]\n",
    "print('Fraud  : ', len(Fraud))\n",
    "print('Normal : ', len(Normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(for_train)  :  227846\n",
      "len(for_test)   :  56961\n"
     ]
    }
   ],
   "source": [
    "# Set X_train equal to 80% of the fraudulent transactions.\n",
    "FraudSample  = Fraud.sample(frac=0.8)\n",
    "NormalSample = Normal.sample(frac=0.8)\n",
    "count_Frauds = len(FraudSample)\n",
    "# Add 80% of the normal transactions to X_train.\n",
    "for_train = pd.concat([FraudSample, NormalSample], axis=0)\n",
    "\n",
    "# X_test contains all the transaction not in X_train.\n",
    "for_test = df.loc[~df.index.isin(for_train.index)]\n",
    "\n",
    "print('len(for_train)  : ',len(for_train))\n",
    "print('len(for_test)   : ',len(for_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(X_train) :  227846\n",
      "len(y_train) :  227846\n",
      "len(X_test)  :  56961\n",
      "len(y_test)  :  56961\n"
     ]
    }
   ],
   "source": [
    "#Shuffle the dataframes so that the training is done in a random order.\n",
    "for_train = for_train.sample(frac=1).reset_index(drop=True)\n",
    "for_test = for_test.sample(frac=1).reset_index(drop=True)\n",
    "#for_test = np.random.shuffle(for_test)\n",
    "\n",
    "# Add our target features to y_train and y_test.\n",
    "X_train = for_train.drop(['Fraud', 'Normal'], axis = 1)\n",
    "# Drop target features from X_train and X_test.\n",
    "#  Fraud, Normal 컬럼 drop\n",
    "y_train = for_train[['Fraud', 'Normal']]\n",
    "\n",
    "# Add our target features to y_train and y_test.\n",
    "X_test = for_test.drop(['Fraud', 'Normal'], axis = 1)\n",
    "# Drop target features from X_train and X_test.\n",
    "#  Fraud, Normal 컬럼 drop\n",
    "y_test = for_test[['Fraud', 'Normal']]\n",
    "\n",
    "#Check to ensure all of the training/testing dataframes are of the correct length\n",
    "print('len(X_train) : ',len(X_train))\n",
    "print('len(y_train) : ',len(y_train))\n",
    "print('len(X_test)  : ',len(X_test))\n",
    "print('len(y_test)  : ',len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio : 578.2893401015228\n",
      "features :  ['Time' 'V1' 'V2' 'V3' 'V4' 'V5' 'V6' 'V7' 'V8' 'V9' 'V10' 'V11' 'V12'\n",
      " 'V13' 'V14' 'V15' 'V16' 'V17' 'V18' 'V19' 'V20' 'V21' 'V22' 'V23' 'V24'\n",
      " 'V25' 'V26' 'V27' 'V28' 'Amount']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/pandas/core/generic.py:3110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "# In [26]\n",
    "'''\n",
    "Due to the imbalance in the data, ratio will act as an equal weighting system for our model. \n",
    "By dividing the number of transactions by those that are fraudulent, ratio will equal the value that when multiplied\n",
    "by the number of fraudulent transactions will equal the number of normal transaction. \n",
    "Simply put: # of fraud * ratio = # of normal\n",
    "'''\n",
    "\n",
    "ratio = len(X_train) / count_Frauds\n",
    "print('ratio :', ratio)\n",
    "\n",
    "y_train.Fraud *= ratio*3\n",
    "y_test.Fraud *= ratio*3\n",
    "\n",
    "#Names of all of the features in X_train.\n",
    "features = X_train.columns.values\n",
    "print('features : ',features)\n",
    "\n",
    "#Transform each feature in features so that it has a mean of 0 and standard deviation of 1;\n",
    "#this helps with training the neural network.\n",
    "for feature in features:\n",
    "    mean, std = df[feature].mean(), df[feature].std()\n",
    "    # print('feature :',feature , 'mean : ', mean , 'std :', std)\n",
    "    X_train.loc[:, feature] = (X_train[feature] - mean) / std\n",
    "    X_test.loc[:, feature] = (X_test[feature] - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split :  28480\n",
      "type(inputX)  :  <class 'numpy.ndarray'>\n",
      "type(X_train) :  <class 'pandas.core.frame.DataFrame'>\n",
      "y_train.Normal.value_counts() : 1.0    227452\n",
      "0.0       394\n",
      "Name: Normal, dtype: int64\n",
      "y_train.Fraud.value_counts() : 0.00000       227452\n",
      "1734.86802       394\n",
      "Name: Fraud, dtype: int64\n",
      "y_test.Normal.value_counts() : 1.0    56863\n",
      "0.0       98\n",
      "Name: Normal, dtype: int64\n",
      "y_test.Fraud.value_counts() : 0.00000       56863\n",
      "1734.86802       98\n",
      "Name: Fraud, dtype: int64\n",
      "C valid_y 41\n",
      "C test_y  57\n",
      "inputX : (227846, 30)\n",
      "inputY : (227846, 2)\n",
      "valid_x : (28480, 30)\n",
      "valid_y : (28480, 2)\n",
      "test_x : (28481, 30)\n",
      "test_y : (28481, 2)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Train the Neural Net\n",
    "'''\n",
    "\n",
    "# In [28]\n",
    "# Split the testing data into validation and testing sets\n",
    "split = int(len(y_test)/2)\n",
    "print('split : ', split)\n",
    "\n",
    "train_x = X_train.as_matrix()\n",
    "train_y = y_train.as_matrix()\n",
    "valid_x = X_test.as_matrix()[:split]\n",
    "valid_y = y_test.as_matrix()[:split]\n",
    "test_x = X_test.as_matrix()[split:]\n",
    "test_y = y_test.as_matrix()[split:]\n",
    "\n",
    "print('type(inputX)  : ', type(train_x))\n",
    "print('type(X_train) : ', type(X_train))\n",
    "\n",
    "print('y_train.Normal.value_counts() :',y_train.Normal.value_counts())\n",
    "print('y_train.Fraud.value_counts() :',y_train.Fraud.value_counts())\n",
    "\n",
    "print('y_test.Normal.value_counts() :',y_test.Normal.value_counts())\n",
    "print('y_test.Fraud.value_counts() :',y_test.Fraud.value_counts())\n",
    "\n",
    "print('C valid_y', np.where(valid_y[:, 0] > 0, 1, 0).sum())\n",
    "print('C test_y ', np.where(test_y[:, 0] > 0, 1, 0).sum())\n",
    "\n",
    "print('inputX :',train_x.shape)\n",
    "print('inputY :',train_y.shape)\n",
    "print('valid_x :',valid_x.shape)\n",
    "print('valid_y :',valid_y.shape)\n",
    "print('test_x :',test_x.shape)\n",
    "print('test_y :',test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "def get_conf_rate(model, testX, testY):\n",
    "    # 모델 예측\n",
    "    predicted_y = model.predict(testX)\n",
    "\n",
    "    # compare predicted_y & testY\n",
    "    conf_cnt = collections.defaultdict(int)\n",
    "    for pr_y, real_y in zip(predicted_y, testY):\n",
    "        conf = np.argmax(pr_y), np.argmax(real_y)\n",
    "        conf_cnt[conf] += 1\n",
    "\n",
    "    # 0번방 == Fraud, 1번방 == Normal\n",
    "    TP = conf_cnt[(0 ,0)]\n",
    "    FN = conf_cnt[(1, 0)]\n",
    "    TN = conf_cnt[(1, 1)]\n",
    "    FP = conf_cnt[(0, 1)]\n",
    "\n",
    "    Acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "    try:\n",
    "        Precision = TP / (TP + FP)\n",
    "    except: # 학습초기에 decision by zero 발생 가능 (범죄라고 판단한 건수가 zero인 상황\n",
    "        Precision = 0\n",
    "      \n",
    "    try:\n",
    "        Recall = TP / (TP + FN)\n",
    "    except:\n",
    "        Recall = 0\n",
    "        raise\n",
    "\n",
    "    print('TP :', TP, ' FN :',FN , ' TN :', TN, ' FP :', FP)\n",
    "    print('Recall: %.3f, Precision: %.3f, Acc: %.3f' % (Recall, Precision, Acc))\n",
    "    return Recall, Precision, Acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Callback Class\n",
    "class mykerasCB(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super(mykerasCB, self).__init__()\n",
    "        self.hist = [] # confusion matrix값 (비율)을 기록하기 위한 것\n",
    "\n",
    "    def on_epoch_end(self, arga, argb):\n",
    "        # arga: 49 -- epoch count\n",
    "        # argb: {'loss': 288.23164164477009, 'acc': 0.98443246759461789}\n",
    "        #print('self.model', self.model)\n",
    "        #print('arga :', arga)\n",
    "        #print('argb :', argb)\n",
    "        if arga % 10 == 9: # 10번 epoch 마다 출력해보기\n",
    "          confusion = get_conf_rate(model, test_x, test_y)\n",
    "          self.hist.append(confusion)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "      \n",
    "# Number of input nodes.\n",
    "input_nodes = train_x.shape[1]\n",
    "\n",
    "# Multiplier maintains a fixed ratio of nodes between each layer.\n",
    "mulitplier = 1.5\n",
    "\n",
    "# Number of nodes in each hidden layer\n",
    "hidden_nodes1 = 18\n",
    "hidden_nodes2 = round(hidden_nodes1 * mulitplier)\n",
    "hidden_nodes3 = round(hidden_nodes2 * mulitplier)\n",
    "\n",
    "# In [31]\n",
    "# Parameters\n",
    "training_epochs = 50             # should be 2000, it will timeout when uploading\n",
    "training_dropout = 0.9          # drop out\n",
    "display_step = 1                # 10\n",
    "n_samples = y_train.shape[0]\n",
    "batch_size = 2048\n",
    "learning_rate = 0.005           # 하이퍼파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================\n",
      "======================  Keras   ======================\n",
      "======================================================\n"
     ]
    }
   ],
   "source": [
    "print('======================================================')\n",
    "print('======================  Keras   ======================')\n",
    "print('======================================================')\n",
    "\n",
    "# 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Dense(hidden_nodes1, input_dim=input_nodes, activation='relu'))\n",
    "model.add(Dense(hidden_nodes2, activation='tanh'))\n",
    "model.add(Dense(hidden_nodes3, activation='relu'))\n",
    "model.add(Dense(hidden_nodes3, activation='sigmoid'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Callback Class 선언\n",
    "cb = mykerasCB()\n",
    "\n",
    "# 모델 학습과정 설정하기\n",
    "model.compile(optimizer='Adam', loss='mean_squared_error', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "227846/227846 [==============================] - 1s 6us/step - loss: 2600.1959 - acc: 0.1708\n",
      "Epoch 2/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.6401 - acc: 0.8462\n",
      "Epoch 3/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.5367 - acc: 0.8856\n",
      "Epoch 4/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.5036 - acc: 0.9060\n",
      "Epoch 5/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.4873 - acc: 0.9187\n",
      "Epoch 6/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.4703 - acc: 0.9190\n",
      "Epoch 7/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.4552 - acc: 0.9200\n",
      "Epoch 8/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.4461 - acc: 0.9353\n",
      "Epoch 9/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.4353 - acc: 0.9413\n",
      "Epoch 10/50\n",
      "223232/227846 [============================>.] - ETA: 0s - loss: 2552.1509 - acc: 0.9430TP : 55  FN : 2  TN : 26300  FP : 2124\n",
      "Recall: 0.965, Precision: 0.025, Acc: 0.925\n",
      "227846/227846 [==============================] - 2s 8us/step - loss: 2599.4281 - acc: 0.9426\n",
      "Epoch 11/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.4191 - acc: 0.9408\n",
      "Epoch 12/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.4091 - acc: 0.9499\n",
      "Epoch 13/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.4042 - acc: 0.9540\n",
      "Epoch 14/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.4018 - acc: 0.9547\n",
      "Epoch 15/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.3925 - acc: 0.9579\n",
      "Epoch 16/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.3910 - acc: 0.9588\n",
      "Epoch 17/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.3829 - acc: 0.9668\n",
      "Epoch 18/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.3808 - acc: 0.9685\n",
      "Epoch 19/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.3763 - acc: 0.9668\n",
      "Epoch 20/50\n",
      "223232/227846 [============================>.] - ETA: 0s - loss: 2605.9706 - acc: 0.9617TP : 53  FN : 4  TN : 27512  FP : 912\n",
      "Recall: 0.930, Precision: 0.055, Acc: 0.968\n",
      "227846/227846 [==============================] - 2s 9us/step - loss: 2599.3795 - acc: 0.9618\n",
      "Epoch 21/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.3755 - acc: 0.9679\n",
      "Epoch 22/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.3721 - acc: 0.9702\n",
      "Epoch 23/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.3714 - acc: 0.9707\n",
      "Epoch 24/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.3761 - acc: 0.9721\n",
      "Epoch 25/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.3755 - acc: 0.9688\n",
      "Epoch 26/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.3721 - acc: 0.9699\n",
      "Epoch 27/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.3696 - acc: 0.9745\n",
      "Epoch 28/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.3766 - acc: 0.9641\n",
      "Epoch 29/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.3660 - acc: 0.9747\n",
      "Epoch 30/50\n",
      "223232/227846 [============================>.] - ETA: 0s - loss: 2599.2258 - acc: 0.9783TP : 53  FN : 4  TN : 27728  FP : 696\n",
      "Recall: 0.930, Precision: 0.071, Acc: 0.975\n",
      "227846/227846 [==============================] - 2s 8us/step - loss: 2599.3682 - acc: 0.9783\n",
      "Epoch 31/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.3653 - acc: 0.9779\n",
      "Epoch 32/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.3650 - acc: 0.9756\n",
      "Epoch 33/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.3638 - acc: 0.9777\n",
      "Epoch 34/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.3608 - acc: 0.9814\n",
      "Epoch 35/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.3595 - acc: 0.9816\n",
      "Epoch 36/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.3596 - acc: 0.9819\n",
      "Epoch 37/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.3570 - acc: 0.9837\n",
      "Epoch 38/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.3614 - acc: 0.9807\n",
      "Epoch 39/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.3620 - acc: 0.9795\n",
      "Epoch 40/50\n",
      "221184/227846 [============================>.] - ETA: 0s - loss: 2609.7005 - acc: 0.9769TP : 54  FN : 3  TN : 27765  FP : 659\n",
      "Recall: 0.947, Precision: 0.076, Acc: 0.977\n",
      "227846/227846 [==============================] - 2s 8us/step - loss: 2599.3680 - acc: 0.9771\n",
      "Epoch 41/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.3733 - acc: 0.9732\n",
      "Epoch 42/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.3604 - acc: 0.9802\n",
      "Epoch 43/50\n",
      "227846/227846 [==============================] - 1s 6us/step - loss: 2599.3573 - acc: 0.9835\n",
      "Epoch 44/50\n",
      "227846/227846 [==============================] - 1s 6us/step - loss: 2599.3547 - acc: 0.9867\n",
      "Epoch 45/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.3654 - acc: 0.9810\n",
      "Epoch 46/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.3745 - acc: 0.9676\n",
      "Epoch 47/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.3640 - acc: 0.9766\n",
      "Epoch 48/50\n",
      "227846/227846 [==============================] - 1s 5us/step - loss: 2599.3618 - acc: 0.9812\n",
      "Epoch 49/50\n",
      "227846/227846 [==============================] - 1s 6us/step - loss: 2599.3595 - acc: 0.9815\n",
      "Epoch 50/50\n",
      "221184/227846 [============================>.] - ETA: 0s - loss: 2575.7252 - acc: 0.9730TP : 53  FN : 4  TN : 27373  FP : 1051\n",
      "Recall: 0.930, Precision: 0.048, Acc: 0.963\n",
      "227846/227846 [==============================] - 2s 10us/step - loss: 2599.3728 - acc: 0.9726\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습시키기\n",
    "hist = model.fit(train_x, train_y, epochs=training_epochs, batch_size=batch_size,\n",
    "                 callbacks = [cb], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP : 44  FN : 6  TN : 27371  FP : 1060\n",
      "Recall: 0.880, Precision: 0.040, Acc: 0.963\n",
      "28480/28480 [==============================] - 0s 3us/step\n",
      "loss_and_metrics : [2533.6159086420294, 0.96327247191011234]\n"
     ]
    }
   ],
   "source": [
    "# 최종 confusion matrix 구하기\n",
    "get_conf_rate(model, test_x, test_y)\n",
    "\n",
    "# 모델 평가하기\n",
    "loss_and_metrics = model.evaluate(valid_x, valid_y, batch_size=batch_size)\n",
    "print('loss_and_metrics : ' + str(loss_and_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1ff13ca128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================\n",
      "======================  Keras   ======================\n",
      "======================================================\n"
     ]
    }
   ],
   "source": [
    "fig, ax =  plt.subplots()\n",
    "\n",
    "acc = [h[0] for h in cb.hist]\n",
    "pre = [h[1] for h in cb.hist]\n",
    "recall = [h[2] for h in cb.hist]\n",
    "\n",
    "ax.plot(acc, label='acc')\n",
    "ax.plot(pre, label='pre')\n",
    "ax.plot(recall, label='recall')\n",
    "\n",
    "ax.set_xlabel('epoch')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('======================================================')\n",
    "print('======================  Keras   ======================')\n",
    "print('======================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
