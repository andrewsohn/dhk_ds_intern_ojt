{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Andrew-MB/DEV/08.PYTHON/01.WORKSPACE/slowcampus0202/env/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/Users/Andrew-MB/DEV/08.PYTHON/01.WORKSPACE/slowcampus0202/env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = w * x + b 함수\n",
    "def myfunc(x):\n",
    "  w = 2.3 # 기울기\n",
    "  b = 3.6 # y 절편.  점(0, 2.6)\n",
    "  # x 절편은 점(-2, 0)이 됨.\n",
    "  y = w * x + b\n",
    "  noise = random.random() * 0.2 # Noise\n",
    "  return y + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.random() -- 0.0 ~ 1.0\n",
    "NUM_DATA = 20          # 데이터 갯수\n",
    "XVALUE = 5              # X값의 범위 (0.0 ~ 5.0)\n",
    "# type: python list\n",
    "xtrain = [random.random() * XVALUE for i in range(NUM_DATA)]\n",
    "xtrain.sort()            # sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain : [0.03934402017340355, 0.23073381000664694, 0.3191599464288458]\n",
      "ytrain : [3.8823997565672506, 4.237665613529607, 4.502078315349286]\n"
     ]
    }
   ],
   "source": [
    "ytrain = [myfunc(x) for x in xtrain]\n",
    "print('xtrain :',xtrain[:3])\n",
    "print('ytrain :',ytrain[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x120d14cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# actual data 산점도\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(xtrain, ytrain, label='actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xlist.shape : (20,)\n",
      "ylist.shape : (20,)\n"
     ]
    }
   ],
   "source": [
    "# type 변환 python list -> numpy ndarray\n",
    "# type: numpy ndarray\n",
    "xtrain = np.array(xtrain)\n",
    "ytrain = np.array(ytrain)\n",
    "print('xlist.shape :',xtrain.shape)  # shape ==  (10,)\n",
    "print('ylist.shape :',ytrain.shape)  # shape ==  (10,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-946c45f9b2eb>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-946c45f9b2eb>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    ytrain = ytrain.???((NUM_DATA, 1))  # shape ==  (10,1)\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# tensor 에 맞추기 위해 reshape\n",
    "xtrain = xtrain.reshape((NUM_DATA, 1))  # shape ==  (10,1)\n",
    "ytrain = ytrain.???((NUM_DATA, 1))  # shape ==  (10,1)\n",
    "print('xlist.shape :', xtrain.shape)\n",
    "print('ylist.shape :', ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 1])\n",
    "Y = tf.???(tf.float32, [None, 1])\n",
    "W = tf.Variable(tf.random_normal([1,1], -1, 1), name='weight')\n",
    "B = tf.???(tf.random_normal([1], -1, 1), name='bias')\n",
    "Y2 = tf.matmul(X, W) + B   # [1,1]행렬 x [1,1]행렬 + [1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 함수\n",
    "# MSE = Mean Squared Error. 오차 제곱의 평균. (Y2-Y)**2의 평균\n",
    "cost_function = tf.reduce_mean(tf.square(Y2 - Y))\n",
    "# 옵티마이저\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=0.1)\n",
    "training = optimizer.minimize(cost_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss list\n",
    "history = []           # Record loss values for plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0000] LOSS 148.383 W [[ 4.06307125]] bias [ 1.9582938]\n",
      "[0001] LOSS 7.82907 W [[ 2.85126829]] bias [ 1.63057256]\n",
      "[0002] LOSS 1.51402 W [[ 3.05496788]] bias [ 1.8320111]\n",
      "[0003] LOSS 1.11175 W [[ 2.96473575]] bias [ 1.9152298]\n",
      "[0004] LOSS 0.982619 W [[ 2.93872929]] bias [ 2.01632619]\n",
      "[0005] LOSS 0.877413 W [[ 2.90173244]] bias [ 2.10715294]\n",
      "[0006] LOSS 0.78391 W [[ 2.86937475]] bias [ 2.19396853]\n",
      "[0007] LOSS 0.700434 W [[ 2.83825183]] bias [ 2.27580047]\n",
      "[0008] LOSS 0.625891 W [[ 2.80895567]] bias [ 2.35317326]\n",
      "[0009] LOSS 0.559326 W [[ 2.78124762]] bias [ 2.42627978]\n",
      "[0010] LOSS 0.499885 W [[ 2.75506902]] bias [ 2.49536562]\n",
      "[0011] LOSS 0.446805 W [[ 2.73032975]] bias [ 2.56064963]\n",
      "[0012] LOSS 0.399406 W [[ 2.7069521]] bias [ 2.62234163]\n",
      "[0013] LOSS 0.357079 W [[ 2.68486071]] bias [ 2.68063927]\n",
      "[0014] LOSS 0.319282 W [[ 2.66398478]] bias [ 2.73572922]\n",
      "[0015] LOSS 0.285529 W [[ 2.64425755]] bias [ 2.78778791]\n",
      "[0016] LOSS 0.255389 W [[ 2.62561584]] bias [ 2.83698225]\n",
      "[0017] LOSS 0.228475 W [[ 2.60799956]] bias [ 2.88346958]\n",
      "[0018] LOSS 0.204441 W [[ 2.59135294]] bias [ 2.92739916]\n",
      "[0019] LOSS 0.182979 W [[ 2.57562208]] bias [ 2.96891165]\n",
      "[0020] LOSS 0.163813 W [[ 2.56075668]] bias [ 3.00813985]\n",
      "[0021] LOSS 0.146699 W [[ 2.54670954]] bias [ 3.04520965]\n",
      "[0022] LOSS 0.131417 W [[ 2.53343511]] bias [ 3.08023977]\n",
      "[0023] LOSS 0.117769 W [[ 2.52089119]] bias [ 3.11334252]\n",
      "[0024] LOSS 0.105583 W [[ 2.50903726]] bias [ 3.14462376]\n",
      "[0025] LOSS 0.0947003 W [[ 2.49783564]] bias [ 3.17418385]\n",
      "[0026] LOSS 0.0849824 W [[ 2.48725057]] bias [ 3.20211744]\n",
      "[0027] LOSS 0.0763046 W [[ 2.47724771]] bias [ 3.22851396]\n",
      "[0028] LOSS 0.0685555 W [[ 2.46779537]] bias [ 3.25345826]\n",
      "[0029] LOSS 0.0616356 W [[ 2.45886278]] bias [ 3.27702999]\n",
      "[0030] LOSS 0.0554563 W [[ 2.45042205]] bias [ 3.29930472]\n",
      "[0031] LOSS 0.0499383 W [[ 2.44244552]] bias [ 3.32035375]\n",
      "[0032] LOSS 0.0450108 W [[ 2.43490815]] bias [ 3.34024477]\n",
      "[0033] LOSS 0.0406107 W [[ 2.42778516]] bias [ 3.35904121]\n",
      "[0034] LOSS 0.0366814 W [[ 2.42105436]] bias [ 3.3768034]\n",
      "[0035] LOSS 0.0331727 W [[ 2.41469407]] bias [ 3.3935883]\n",
      "[0036] LOSS 0.0300395 W [[ 2.40868354]] bias [ 3.40944958]\n",
      "[0037] LOSS 0.0272415 W [[ 2.40300369]] bias [ 3.42443824]\n",
      "[0038] LOSS 0.024743 W [[ 2.39763618]] bias [ 3.43860197]\n",
      "[0039] LOSS 0.0225119 W [[ 2.3925643]] bias [ 3.45198655]\n",
      "[0040] LOSS 0.0205195 W [[ 2.38777161]] bias [ 3.46463466]\n",
      "[0041] LOSS 0.0187405 W [[ 2.38324213]] bias [ 3.47658682]\n",
      "[0042] LOSS 0.0171517 W [[ 2.37896228]] bias [ 3.48788142]\n",
      "[0043] LOSS 0.015733 W [[ 2.37491775]] bias [ 3.49855447]\n",
      "[0044] LOSS 0.0144661 W [[ 2.3710959]] bias [ 3.50864029]\n",
      "[0045] LOSS 0.0133348 W [[ 2.36748433]] bias [ 3.51817107]\n",
      "[0046] LOSS 0.0123246 W [[ 2.36407137]] bias [ 3.52717733]\n",
      "[0047] LOSS 0.0114225 W [[ 2.36084628]] bias [ 3.53568816]\n",
      "[0048] LOSS 0.0106169 W [[ 2.35779858]] bias [ 3.54373074]\n",
      "[0049] LOSS 0.00989754 W [[ 2.35491848]] bias [ 3.5513308]\n",
      "[0050] LOSS 0.00925515 W [[ 2.35219717]] bias [ 3.55851269]\n",
      "[0051] LOSS 0.00868153 W [[ 2.34962535]] bias [ 3.56529927]\n",
      "[0052] LOSS 0.00816931 W [[ 2.34719515]] bias [ 3.57171249]\n",
      "[0053] LOSS 0.00771187 W [[ 2.34489846]] bias [ 3.57777286]\n",
      "[0054] LOSS 0.00730341 W [[ 2.34272838]] bias [ 3.58349991]\n",
      "[0055] LOSS 0.00693863 W [[ 2.3406775]] bias [ 3.58891177]\n",
      "[0056] LOSS 0.00661293 W [[ 2.33873963]] bias [ 3.59402585]\n",
      "[0057] LOSS 0.00632206 W [[ 2.33690834]] bias [ 3.59885859]\n",
      "[0058] LOSS 0.0060623 W [[ 2.33517766]] bias [ 3.60342526]\n",
      "[0059] LOSS 0.00583038 W [[ 2.33354259]] bias [ 3.60774088]\n",
      "[0060] LOSS 0.00562328 W [[ 2.33199692]] bias [ 3.61181879]\n",
      "[0061] LOSS 0.00543832 W [[ 2.33053684]] bias [ 3.61567259]\n",
      "[0062] LOSS 0.00527315 W [[ 2.32915688]] bias [ 3.61931419]\n",
      "[0063] LOSS 0.00512565 W [[ 2.32785273]] bias [ 3.62275529]\n",
      "[0064] LOSS 0.00499397 W [[ 2.32662058]] bias [ 3.62600732]\n",
      "[0065] LOSS 0.00487635 W [[ 2.3254559]] bias [ 3.6290803]\n",
      "[0066] LOSS 0.00477133 W [[ 2.3243556]] bias [ 3.63198423]\n",
      "[0067] LOSS 0.00467755 W [[ 2.32331562]] bias [ 3.63472843]\n",
      "[0068] LOSS 0.0045938 W [[ 2.3223331]] bias [ 3.63732147]\n",
      "[0069] LOSS 0.00451903 W [[ 2.32140446]] bias [ 3.63977194]\n",
      "[0070] LOSS 0.00445224 W [[ 2.32052708]] bias [ 3.64208746]\n",
      "[0071] LOSS 0.00439263 W [[ 2.31969786]] bias [ 3.64427567]\n",
      "[0072] LOSS 0.00433936 W [[ 2.31891418]] bias [ 3.64634347]\n",
      "[0073] LOSS 0.00429182 W [[ 2.31817389]] bias [ 3.64829755]\n",
      "[0074] LOSS 0.00424936 W [[ 2.31747413]] bias [ 3.6501441]\n",
      "[0075] LOSS 0.00421141 W [[ 2.31681275]] bias [ 3.65188885]\n",
      "[0076] LOSS 0.00417757 W [[ 2.3161881]] bias [ 3.65353775]\n",
      "[0077] LOSS 0.00414733 W [[ 2.31559753]] bias [ 3.65509582]\n",
      "[0078] LOSS 0.00412031 W [[ 2.31503963]] bias [ 3.65656829]\n",
      "[0079] LOSS 0.00409621 W [[ 2.31451225]] bias [ 3.6579597]\n",
      "[0080] LOSS 0.00407469 W [[ 2.3140142]] bias [ 3.65927458]\n",
      "[0081] LOSS 0.00405546 W [[ 2.31354308]] bias [ 3.66051698]\n",
      "[0082] LOSS 0.00403829 W [[ 2.31309843]] bias [ 3.66169119]\n",
      "[0083] LOSS 0.00402294 W [[ 2.31267786]] bias [ 3.66280055]\n",
      "[0084] LOSS 0.00400926 W [[ 2.31228065]] bias [ 3.66384912]\n",
      "[0085] LOSS 0.00399705 W [[ 2.31190515]] bias [ 3.66483998]\n",
      "[0086] LOSS 0.00398613 W [[ 2.31155038]] bias [ 3.66577625]\n",
      "[0087] LOSS 0.00397638 W [[ 2.31121492]] bias [ 3.66666102]\n",
      "[0088] LOSS 0.00396765 W [[ 2.3108983]] bias [ 3.66749716]\n",
      "[0089] LOSS 0.0039599 W [[ 2.31059885]] bias [ 3.66828728]\n",
      "[0090] LOSS 0.00395295 W [[ 2.31031585]] bias [ 3.66903377]\n",
      "[0091] LOSS 0.00394675 W [[ 2.31004858]] bias [ 3.66973925]\n",
      "[0092] LOSS 0.00394123 W [[ 2.30979586]] bias [ 3.67040586]\n",
      "[0093] LOSS 0.00393626 W [[ 2.3095572]] bias [ 3.67103601]\n",
      "[0094] LOSS 0.00393186 W [[ 2.30933166]] bias [ 3.67163134]\n",
      "[0095] LOSS 0.00392792 W [[ 2.30911827]] bias [ 3.67219377]\n",
      "[0096] LOSS 0.00392441 W [[ 2.30891705]] bias [ 3.67272544]\n",
      "[0097] LOSS 0.00392125 W [[ 2.30872655]] bias [ 3.67322779]\n",
      "[0098] LOSS 0.00391845 W [[ 2.30854678]] bias [ 3.67370248]\n",
      "[0099] LOSS 0.00391594 W [[ 2.30837679]] bias [ 3.67415118]\n",
      "[0100] LOSS 0.00391371 W [[ 2.30821609]] bias [ 3.67457509]\n",
      "[0101] LOSS 0.00391171 W [[ 2.30806446]] bias [ 3.67497563]\n",
      "[0102] LOSS 0.00390993 W [[ 2.30792093]] bias [ 3.67535424]\n",
      "[0103] LOSS 0.00390833 W [[ 2.30778527]] bias [ 3.67571187]\n",
      "[0104] LOSS 0.00390691 W [[ 2.30765724]] bias [ 3.67604995]\n",
      "[0105] LOSS 0.00390562 W [[ 2.30753613]] bias [ 3.67636943]\n",
      "[0106] LOSS 0.00390449 W [[ 2.30742168]] bias [ 3.67667127]\n",
      "[0107] LOSS 0.00390349 W [[ 2.30731368]] bias [ 3.67695665]\n",
      "[0108] LOSS 0.00390258 W [[ 2.3072114]] bias [ 3.67722631]\n",
      "[0109] LOSS 0.00390177 W [[ 2.30711484]] bias [ 3.67748117]\n",
      "[0110] LOSS 0.00390105 W [[ 2.30702376]] bias [ 3.67772198]\n",
      "[0111] LOSS 0.0039004 W [[ 2.30693746]] bias [ 3.67794943]\n",
      "[0112] LOSS 0.00389983 W [[ 2.30685592]] bias [ 3.67816424]\n",
      "[0113] LOSS 0.00389932 W [[ 2.30677891]] bias [ 3.67836738]\n",
      "[0114] LOSS 0.00389886 W [[ 2.30670619]] bias [ 3.6785593]\n",
      "[0115] LOSS 0.00389845 W [[ 2.30663753]] bias [ 3.67874074]\n",
      "[0116] LOSS 0.00389808 W [[ 2.30657268]] bias [ 3.67891216]\n",
      "[0117] LOSS 0.00389775 W [[ 2.30651116]] bias [ 3.67907405]\n",
      "[0118] LOSS 0.00389746 W [[ 2.30645323]] bias [ 3.67922711]\n",
      "[0119] LOSS 0.00389721 W [[ 2.30639839]] bias [ 3.67937183]\n",
      "[0120] LOSS 0.00389696 W [[ 2.30634665]] bias [ 3.67950845]\n",
      "[0121] LOSS 0.00389676 W [[ 2.30629778]] bias [ 3.67963767]\n",
      "[0122] LOSS 0.00389657 W [[ 2.30625129]] bias [ 3.67975974]\n",
      "[0123] LOSS 0.0038964 W [[ 2.30620766]] bias [ 3.67987514]\n",
      "[0124] LOSS 0.00389626 W [[ 2.30616641]] bias [ 3.67998409]\n",
      "[0125] LOSS 0.00389613 W [[ 2.30612731]] bias [ 3.68008709]\n",
      "[0126] LOSS 0.00389601 W [[ 2.30609059]] bias [ 3.68018436]\n",
      "[0127] LOSS 0.00389589 W [[ 2.30605555]] bias [ 3.68027639]\n",
      "[0128] LOSS 0.00389583 W [[ 2.30602264]] bias [ 3.68036342]\n",
      "[0129] LOSS 0.00389572 W [[ 2.30599141]] bias [ 3.68044543]\n",
      "[0130] LOSS 0.00389565 W [[ 2.30596209]] bias [ 3.68052316]\n",
      "[0131] LOSS 0.0038956 W [[ 2.30593419]] bias [ 3.68059659]\n",
      "[0132] LOSS 0.00389553 W [[ 2.30590796]] bias [ 3.68066597]\n",
      "[0133] LOSS 0.00389546 W [[ 2.30588317]] bias [ 3.68073153]\n",
      "[0134] LOSS 0.00389543 W [[ 2.30585957]] bias [ 3.68079329]\n",
      "[0135] LOSS 0.00389538 W [[ 2.30583763]] bias [ 3.68085194]\n",
      "[0136] LOSS 0.00389534 W [[ 2.30581665]] bias [ 3.68090725]\n",
      "[0137] LOSS 0.00389531 W [[ 2.30579662]] bias [ 3.68095946]\n",
      "[0138] LOSS 0.00389528 W [[ 2.30577803]] bias [ 3.68100882]\n",
      "[0139] LOSS 0.00389527 W [[ 2.30576038]] bias [ 3.68105531]\n",
      "[0140] LOSS 0.00389523 W [[ 2.30574369]] bias [ 3.68109941]\n",
      "[0141] LOSS 0.00389518 W [[ 2.30572796]] bias [ 3.68114114]\n",
      "[0142] LOSS 0.00389519 W [[ 2.30571294]] bias [ 3.68118048]\n",
      "[0143] LOSS 0.00389515 W [[ 2.30569887]] bias [ 3.68121767]\n",
      "[0144] LOSS 0.00389517 W [[ 2.30568552]] bias [ 3.68125272]\n",
      "[0145] LOSS 0.00389514 W [[ 2.30567312]] bias [ 3.68128586]\n",
      "[0146] LOSS 0.00389514 W [[ 2.3056612]] bias [ 3.68131733]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0147] LOSS 0.00389512 W [[ 2.30564976]] bias [ 3.68134689]\n",
      "[0148] LOSS 0.00389511 W [[ 2.30563927]] bias [ 3.68137503]\n",
      "[0149] LOSS 0.0038951 W [[ 2.30562925]] bias [ 3.68140149]\n",
      "[0150] LOSS 0.00389508 W [[ 2.30561972]] bias [ 3.68142653]\n",
      "[0151] LOSS 0.00389509 W [[ 2.30561066]] bias [ 3.68145013]\n",
      "[0152] LOSS 0.00389508 W [[ 2.30560231]] bias [ 3.68147254]\n",
      "[0153] LOSS 0.00389507 W [[ 2.30559444]] bias [ 3.68149376]\n",
      "[0154] LOSS 0.00389508 W [[ 2.30558681]] bias [ 3.68151355]\n",
      "[0155] LOSS 0.00389506 W [[ 2.30557966]] bias [ 3.68153238]\n",
      "[0156] LOSS 0.00389506 W [[ 2.30557275]] bias [ 3.68155003]\n",
      "[0157] LOSS 0.00389504 W [[ 2.30556655]] bias [ 3.68156695]\n",
      "[0158] LOSS 0.00389504 W [[ 2.30556035]] bias [ 3.68158293]\n",
      "[0159] LOSS 0.00389504 W [[ 2.30555487]] bias [ 3.68159795]\n",
      "[0160] LOSS 0.00389505 W [[ 2.30554938]] bias [ 3.68161201]\n",
      "[0161] LOSS 0.00389506 W [[ 2.30554438]] bias [ 3.68162537]\n",
      "[0162] LOSS 0.00389504 W [[ 2.30553961]] bias [ 3.681638]\n",
      "[0163] LOSS 0.00389503 W [[ 2.30553508]] bias [ 3.68164992]\n",
      "[0164] LOSS 0.00389503 W [[ 2.30553079]] bias [ 3.68166137]\n",
      "[0165] LOSS 0.00389504 W [[ 2.30552673]] bias [ 3.6816721]\n",
      "[0166] LOSS 0.00389503 W [[ 2.30552292]] bias [ 3.68168211]\n",
      "[0167] LOSS 0.00389503 W [[ 2.30551934]] bias [ 3.68169165]\n",
      "[0168] LOSS 0.00389504 W [[ 2.305516]] bias [ 3.68170071]\n",
      "[0169] LOSS 0.00389504 W [[ 2.30551267]] bias [ 3.68170929]\n",
      "[0170] LOSS 0.00389504 W [[ 2.30550957]] bias [ 3.6817174]\n",
      "[0171] LOSS 0.00389502 W [[ 2.30550671]] bias [ 3.68172503]\n",
      "[0172] LOSS 0.00389503 W [[ 2.30550408]] bias [ 3.68173218]\n",
      "[0173] LOSS 0.00389503 W [[ 2.30550146]] bias [ 3.68173885]\n",
      "[0174] LOSS 0.00389503 W [[ 2.30549884]] bias [ 3.68174529]\n",
      "[0175] LOSS 0.00389504 W [[ 2.30549669]] bias [ 3.68175149]\n",
      "[0176] LOSS 0.00389503 W [[ 2.30549455]] bias [ 3.68175721]\n",
      "[0177] LOSS 0.00389504 W [[ 2.3054924]] bias [ 3.6817627]\n",
      "[0178] LOSS 0.00389502 W [[ 2.30549049]] bias [ 3.68176794]\n",
      "[0179] LOSS 0.00389502 W [[ 2.30548859]] bias [ 3.68177271]\n",
      "[0180] LOSS 0.00389503 W [[ 2.30548692]] bias [ 3.68177724]\n",
      "[0181] LOSS 0.00389505 W [[ 2.30548525]] bias [ 3.68178153]\n",
      "[0182] LOSS 0.00389503 W [[ 2.30548358]] bias [ 3.68178558]\n",
      "[0183] LOSS 0.00389502 W [[ 2.30548215]] bias [ 3.6817894]\n",
      "[0184] LOSS 0.00389501 W [[ 2.30548072]] bias [ 3.68179321]\n",
      "[0185] LOSS 0.00389503 W [[ 2.30547953]] bias [ 3.68179679]\n",
      "[0186] LOSS 0.00389503 W [[ 2.3054781]] bias [ 3.68179989]\n",
      "[0187] LOSS 0.00389503 W [[ 2.30547714]] bias [ 3.68180299]\n",
      "[0188] LOSS 0.00389503 W [[ 2.30547595]] bias [ 3.68180585]\n",
      "[0189] LOSS 0.00389503 W [[ 2.305475]] bias [ 3.68180871]\n",
      "[0190] LOSS 0.00389503 W [[ 2.30547404]] bias [ 3.68181133]\n",
      "[0191] LOSS 0.00389504 W [[ 2.30547309]] bias [ 3.68181372]\n",
      "[0192] LOSS 0.00389503 W [[ 2.30547214]] bias [ 3.6818161]\n",
      "[0193] LOSS 0.00389504 W [[ 2.30547118]] bias [ 3.68181825]\n",
      "[0194] LOSS 0.00389502 W [[ 2.30547047]] bias [ 3.68182039]\n",
      "[0195] LOSS 0.00389502 W [[ 2.30546975]] bias [ 3.6818223]\n",
      "[0196] LOSS 0.00389503 W [[ 2.30546904]] bias [ 3.68182421]\n",
      "[0197] LOSS 0.00389502 W [[ 2.30546832]] bias [ 3.68182588]\n",
      "[0198] LOSS 0.00389503 W [[ 2.30546784]] bias [ 3.68182755]\n",
      "[0199] LOSS 0.00389502 W [[ 2.30546713]] bias [ 3.68182921]\n",
      "[0200] LOSS 0.00389502 W [[ 2.30546665]] bias [ 3.68183064]\n",
      "[0201] LOSS 0.00389504 W [[ 2.30546618]] bias [ 3.68183208]\n",
      "[0202] LOSS 0.00389501 W [[ 2.30546546]] bias [ 3.68183327]\n",
      "[0203] LOSS 0.00389503 W [[ 2.30546522]] bias [ 3.6818347]\n",
      "[0204] LOSS 0.00389503 W [[ 2.30546451]] bias [ 3.68183589]\n",
      "[0205] LOSS 0.00389502 W [[ 2.30546427]] bias [ 3.68183708]\n",
      "[0206] LOSS 0.00389502 W [[ 2.30546379]] bias [ 3.68183804]\n",
      "[0207] LOSS 0.00389502 W [[ 2.30546355]] bias [ 3.68183899]\n",
      "[0208] LOSS 0.00389502 W [[ 2.30546308]] bias [ 3.68183994]\n",
      "[0209] LOSS 0.00389503 W [[ 2.30546284]] bias [ 3.6818409]\n",
      "[0210] LOSS 0.00389503 W [[ 2.30546236]] bias [ 3.68184161]\n",
      "[0211] LOSS 0.00389502 W [[ 2.30546212]] bias [ 3.68184257]\n",
      "[0212] LOSS 0.00389502 W [[ 2.30546188]] bias [ 3.68184328]\n",
      "[0213] LOSS 0.00389503 W [[ 2.30546141]] bias [ 3.681844]\n",
      "[0214] LOSS 0.00389504 W [[ 2.30546141]] bias [ 3.68184471]\n",
      "[0215] LOSS 0.00389504 W [[ 2.30546093]] bias [ 3.68184519]\n",
      "[0216] LOSS 0.00389503 W [[ 2.30546093]] bias [ 3.6818459]\n",
      "[0217] LOSS 0.00389504 W [[ 2.30546069]] bias [ 3.68184638]\n",
      "[0218] LOSS 0.00389502 W [[ 2.30546045]] bias [ 3.68184686]\n",
      "[0219] LOSS 0.00389503 W [[ 2.30546045]] bias [ 3.68184733]\n",
      "[0220] LOSS 0.00389503 W [[ 2.30545998]] bias [ 3.68184781]\n",
      "[0221] LOSS 0.00389504 W [[ 2.30545998]] bias [ 3.68184829]\n",
      "[0222] LOSS 0.00389503 W [[ 2.30545974]] bias [ 3.68184876]\n",
      "[0223] LOSS 0.00389501 W [[ 2.30545974]] bias [ 3.68184924]\n",
      "[0224] LOSS 0.00389502 W [[ 2.30545926]] bias [ 3.68184948]\n",
      "[0225] LOSS 0.00389502 W [[ 2.30545926]] bias [ 3.68184996]\n",
      "[0226] LOSS 0.00389503 W [[ 2.30545902]] bias [ 3.68185019]\n",
      "[0227] LOSS 0.00389502 W [[ 2.30545902]] bias [ 3.68185067]\n",
      "[0228] LOSS 0.00389503 W [[ 2.30545902]] bias [ 3.68185091]\n",
      "[0229] LOSS 0.00389502 W [[ 2.30545878]] bias [ 3.68185115]\n",
      "[0230] LOSS 0.00389502 W [[ 2.30545878]] bias [ 3.68185139]\n",
      "[0231] LOSS 0.00389503 W [[ 2.30545855]] bias [ 3.68185163]\n",
      "[0232] LOSS 0.00389503 W [[ 2.30545855]] bias [ 3.68185186]\n",
      "[0233] LOSS 0.00389504 W [[ 2.30545855]] bias [ 3.6818521]\n",
      "[0234] LOSS 0.00389503 W [[ 2.30545831]] bias [ 3.68185234]\n",
      "[0235] LOSS 0.00389503 W [[ 2.30545831]] bias [ 3.68185258]\n",
      "[0236] LOSS 0.00389502 W [[ 2.30545831]] bias [ 3.68185282]\n",
      "[0237] LOSS 0.00389503 W [[ 2.30545831]] bias [ 3.68185306]\n",
      "[0238] LOSS 0.00389502 W [[ 2.30545807]] bias [ 3.68185306]\n",
      "[0239] LOSS 0.00389502 W [[ 2.30545807]] bias [ 3.68185329]\n",
      "[0240] LOSS 0.00389502 W [[ 2.30545807]] bias [ 3.68185353]\n",
      "[0241] LOSS 0.00389502 W [[ 2.30545807]] bias [ 3.68185377]\n",
      "[0242] LOSS 0.00389503 W [[ 2.30545783]] bias [ 3.68185377]\n",
      "[0243] LOSS 0.00389501 W [[ 2.30545783]] bias [ 3.68185401]\n",
      "[0244] LOSS 0.00389502 W [[ 2.30545759]] bias [ 3.68185401]\n",
      "[0245] LOSS 0.00389501 W [[ 2.30545759]] bias [ 3.68185425]\n",
      "[0246] LOSS 0.00389502 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0247] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0248] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0249] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0250] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0251] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0252] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0253] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0254] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0255] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0256] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0257] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0258] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0259] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0260] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0261] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0262] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0263] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0264] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0265] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0266] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0267] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0268] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0269] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0270] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0271] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0272] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0273] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0274] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0275] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0276] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0277] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0278] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0279] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0280] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0281] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0282] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0283] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0284] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0285] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0286] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0287] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0288] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0289] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0290] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0291] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0292] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0293] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0294] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0295] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0296] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0297] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0298] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0299] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0300] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0301] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0302] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0303] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0304] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0305] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0306] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0307] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0308] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0309] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0310] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0311] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0312] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0313] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0314] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0315] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0316] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0317] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0318] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0319] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0320] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0321] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0322] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0323] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0324] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0325] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0326] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0327] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0328] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0329] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0330] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0331] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0332] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0333] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0334] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0335] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0336] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0337] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0338] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0339] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0340] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0341] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0342] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0343] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0344] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0345] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0346] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0347] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0348] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0349] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0350] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0351] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0352] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0353] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0354] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0355] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0356] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0357] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0358] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0359] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0360] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0361] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0362] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0363] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0364] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0365] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0366] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0367] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0368] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0369] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0370] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0371] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0372] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0373] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0374] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0375] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0376] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0377] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0378] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0379] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0380] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0381] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0382] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0383] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0384] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0385] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0386] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0387] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0388] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0389] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0390] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0391] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0392] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0393] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0394] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0395] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0396] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0397] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0398] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0399] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0400] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0401] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0402] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0403] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0404] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0405] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0406] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0407] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0408] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0409] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0410] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0411] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0412] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0413] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0414] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0415] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0416] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0417] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0418] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0419] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0420] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0421] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0422] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0423] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0424] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0425] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0426] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0427] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0428] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0429] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0430] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0431] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0432] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0433] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0434] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0435] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0436] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0437] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0438] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0439] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0440] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0441] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0442] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0443] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0444] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0445] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0446] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0447] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0448] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0449] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0450] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0451] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0452] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0453] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0454] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0455] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0456] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0457] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0458] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0459] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0460] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0461] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0462] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0463] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0464] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0465] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0466] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0467] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0468] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0469] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0470] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0471] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0472] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0473] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0474] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0475] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0476] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0477] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0478] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0479] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0480] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0481] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0482] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0483] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0484] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0485] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0486] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0487] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0488] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0489] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0490] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0491] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0492] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0493] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0494] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0495] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0496] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0497] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0498] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n",
      "[0499] LOSS 0.00389503 W [[ 2.30545759]] bias [ 3.68185449]\n"
     ]
    }
   ],
   "source": [
    "# Session 시작\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(500):\n",
    "  res, cost  = sess.run([training, cost_function],\n",
    "                        feed_dict={X: xtrain, Y:ytrain})\n",
    "  if cost < 0.003:\n",
    "    break\n",
    "\n",
    "  if i % 1 == 0:\n",
    "    y2, w, b = sess.run([Y2, W, B], feed_dict={X: xtrain, Y: ytrain})\n",
    "    print('[%04d]' % i, 'LOSS', cost, 'W', w, 'bias', b)\n",
    "\n",
    "    history.append(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loss 값의 변화 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f672466a048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the accuracy and cost summaries\n",
    "f, ax = plt.subplots()\n",
    "\n",
    "ax.plot(history[:20])    # GradientDescentOptimizer\n",
    "ax.set_title('optimizer')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Losses')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 데이터에 대한 정확도 확인 (학습 오차 확인)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_from_nn = sess.run([Y2], feed_dict={X: xtrain})\n",
    "\n",
    "for y_nn, y_real in zip(y_from_nn[0], ytrain):\n",
    "    err = abs(y_nn[0] - y_real) / y_real * 100\n",
    "    print('%.2f  %.2f 오차율 %.2f %%' % (y_nn[0], y_real, err))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테스트 데이터로 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_unseen = [random.random() * XVALUE for i in range(NUM_DATA)]\n",
    "x_unseen = np.array(x_unseen).reshape((NUM_DATA, 1))\n",
    "\n",
    "y_unseen = sess.run([Y2], feed_dict={X: x_unseen})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(xtrain, ytrain, label='train')\n",
    "ax.scatter(x_unseen, y_unseen, label='unseen')\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
